{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Lecture 3\n",
    "\n",
    "Today's goal:\n",
    "- (Review) Iterations to prepare the implementation of a simple gradient descent algorithm.\n",
    "- `PyTorch` autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essential components of gradient descent\n",
    "\n",
    "Example: linear regression. Use a cubic polynomial to fit a $f(x) :=\\sin(x)$ function on $D = (-\\pi, \\pi)$. That is:\n",
    "\n",
    "$$\\min_{p\\in \\mathbb{P}^3} \\|p - f\\|^2_{L^2(D)} $$\n",
    "\n",
    "Equivalently, this is approximated as\n",
    "\n",
    "$$\\min_{(a,b,c,d)\\in \\mathbb{R}^4 } \\sum_{x\\in D_h} \\Delta x\\, |a+bx+cx^2+dx^3 - f(x)|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float # single-precision float number\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "delta_x = x[1] - x[0]\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "N_iter = 2000\n",
    "with tqdm(total=N_iter) as pbar:\n",
    "    for t in range(N_iter):\n",
    "        # Forward pass: compute predicted y\n",
    "        y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = (delta_x*(y_pred - y).pow(2).sum())\n",
    "\n",
    "        # Backprop (chain rule) to compute gradients of a, b, c, d with respect to loss\n",
    "        grad_y_pred = 2.0 * delta_x * (y_pred - y)\n",
    "        grad_a = grad_y_pred.sum()\n",
    "        grad_b = (grad_y_pred * x).sum()\n",
    "        grad_c = (grad_y_pred * x ** 2).sum()\n",
    "        grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "        # Update weights using gradient descent\n",
    "        a -= learning_rate * grad_a\n",
    "        b -= learning_rate * grad_b\n",
    "        c -= learning_rate * grad_c\n",
    "        d -= learning_rate * grad_d\n",
    "        pbar.set_description(f\"loss: {loss.item():.6f}\")\n",
    "        pbar.update()\n",
    "        time.sleep(2e-3)\n",
    "\n",
    "\n",
    "print(f'Result: y approx = {a.item():.3f} + {b.item():.3f} x + {c.item():.3f} x^2 + {d.item():.3f} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute gradient in PyTorch: autograd\n",
    "\n",
    "Let's take a look at how ``autograd`` collects gradients. We create two tensors ``a`` and ``b`` with\n",
    "``requires_grad=True``. This signals to ``autograd`` that every operation on them should be \"tracked\".\n",
    "\n",
    "Reference: \n",
    "- adapted from the official tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another tensor ``f`` from ``a`` and ``b`` elementwisely.\n",
    "\n",
    "\\begin{align}f = 3a^3 - b^2\\end{align}\n",
    "\n",
    "or let $f = (f_i)^{\\top}$ and for each $i$\n",
    "\n",
    "\\begin{align}f_i = 3a_i^3 - b_i^2\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 3*a**3 - b**2\n",
    "\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``a`` and ``b`` are usually parameters of a neural network, and ``f``\n",
    "to be the ``loss function`` (similar to the error/residual above). In gradient based methods, we want gradients of each component of $f$ w.r.t. parameters, i.e.\n",
    "\n",
    "\\begin{align}\\frac{\\partial f_i}{\\partial a_i} = 9a_i^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial f_i}{\\partial b_i} = -2b_i\\end{align}\n",
    "\n",
    "\n",
    "When we call ``.backward()`` on ``f``, autograd calculates these gradients\n",
    "and stores them in the respective tensors' ``.grad`` attribute.\n",
    "\n",
    "We need to explicitly pass a ``gradient`` argument in ``f.backward()`` because it is a vector.\n",
    "``gradient`` is a tensor of the same shape as ``f``, and it represents the\n",
    "gradient of $f$ w.r.t. itself, i.e.\n",
    "\n",
    "\\begin{align}\\frac{df_i}{df_i} = 1\\end{align}\n",
    "\n",
    "Equivalently, we can also aggregate $f$ into a scalar and call backward implicitly, like ``f.sum().backward()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.]) # first component and second component modifier\n",
    "f.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are now deposited in ``a.grad`` and ``b.grad``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad = None\n",
    "b.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set $g = \\sum f_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (3*a**3 - b**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of GD using autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "delta_x = x[1] - x[0]\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "N_iter = 2000\n",
    "with tqdm(total=N_iter) as pbar:\n",
    "    for t in range(N_iter):\n",
    "        # Forward pass: compute predicted y\n",
    "        y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "        # for p in [a, b, c, d]:\n",
    "        #     p.grad = None\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = (delta_x*(y_pred - y).pow(2).sum())\n",
    "\n",
    "        # Backprop (chain rule) through autograd\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights using gradient descent\n",
    "        with torch.no_grad(): # this is necessary if autograd attribute is called manually\n",
    "            a -= learning_rate * a.grad\n",
    "            b -= learning_rate * b.grad\n",
    "            c -= learning_rate * c.grad\n",
    "            d -= learning_rate * d.grad\n",
    "        \n",
    "            # manually zeroing the grad stored in each variable\n",
    "            a.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            c.grad.zero_()\n",
    "            d.grad.zero_()\n",
    "        \n",
    "        pbar.set_description(f\"loss: {loss.item():.6f}\")\n",
    "        pbar.update()\n",
    "        time.sleep(2e-3)\n",
    "\n",
    "\n",
    "print(f'Result: y approx = {a.item():.3f} + {b.item():.3f} x + {c.item():.3f} x^2 + {d.item():.3f} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using matmul\n",
    "\n",
    "Data matrix $X$:\n",
    "\n",
    "$$\n",
    "X_{i, \\cdot}  = [1, x_i, x_i^2, x_i^3 ], y_i = f(x_i), \\text{ and } w = [a, b, c, d]^{\\top} \n",
    "$$\n",
    "then the regression becomes \n",
    "$$\n",
    "\\min_{w\\in \\mathbb{R}^4}\\Delta x\\, \\|Xw - y\\|^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
